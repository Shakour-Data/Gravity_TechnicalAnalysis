# ğŸ“š Runbook - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Technical Analysis Microservice

## ğŸ¯ Ù‡Ø¯Ù
Ø§ÛŒÙ† Runbook Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒØŒ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ù…ÛŒÚ©Ø±ÙˆØ³Ø±ÙˆÛŒØ³ Technical Analysis Ø§Ø³Øª.

---

## ğŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨
1. [Ù…Ø¹Ù…Ø§Ø±ÛŒ Ùˆ Ø§Ø¬Ø²Ø§](#Ù…Ø¹Ù…Ø§Ø±ÛŒ-Ùˆ-Ø§Ø¬Ø²Ø§)
2. [Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ùˆ Deployment](#Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ-Ùˆ-deployment)
3. [Monitoring Ùˆ Alerts](#monitoring-Ùˆ-alerts)
4. [Troubleshooting](#troubleshooting)
5. [Backup Ùˆ Recovery](#backup-Ùˆ-recovery)
6. [Scaling Strategies](#scaling-strategies)
7. [Security Procedures](#security-procedures)

---

## ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ùˆ Ø§Ø¬Ø²Ø§

### Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Balancer / Ingress                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   API Gateway  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Technical Analysis   â”‚
    â”‚   Microservice       â”‚
    â”‚  (3-20 replicas)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Redis  â”‚          â”‚  Kafka /  â”‚
â”‚ Cache  â”‚          â”‚ RabbitMQ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ
1. **API Layer**: FastAPI Ø¨Ø§ 3+ replicas
2. **Cache Layer**: Redis cluster
3. **Message Queue**: Kafka/RabbitMQ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
4. **Service Discovery**: Eureka/Consul
5. **Observability**: Prometheus + Jaeger

---

## ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ùˆ Deployment

### Prerequisites
- Kubernetes cluster (v1.25+)
- kubectl configured
- Helm 3.x (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
- Docker registry access

### Deployment Steps

#### 1. ØªÙ†Ø¸ÛŒÙ… Namespace
```bash
kubectl apply -f k8s/namespace.yaml
```

#### 2. ØªÙ†Ø¸ÛŒÙ… Secrets
```bash
# Ø§ÛŒØ¬Ø§Ø¯ secret Ø¨Ø±Ø§ÛŒ credentials
kubectl create secret generic technical-analysis-secret \
  --from-literal=SECRET_KEY="your-secret-key" \
  --from-literal=REDIS_PASSWORD="redis-password" \
  -n tech-analysis-prod

# Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Vault (ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯):
kubectl apply -f k8s/vault-secret-sync.yaml
```

#### 3. Deploy Redis
```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install redis bitnami/redis \
  --namespace tech-analysis-prod \
  --set auth.password=secure-password \
  --set master.persistence.size=10Gi \
  --set replica.replicaCount=2
```

#### 4. Deploy Application
```bash
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/rbac.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/hpa.yaml
kubectl apply -f k8s/ingress.yaml
```

#### 5. Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª
```bash
# ÙˆØ¶Ø¹ÛŒØª pods
kubectl get pods -n tech-analysis-prod

# logs
kubectl logs -f deployment/technical-analysis -n tech-analysis-prod

# health check
kubectl run test --rm -i --restart=Never \
  --image=curlimages/curl \
  -- curl http://technical-analysis-service:8000/health
```

### Rolling Update
```bash
# Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ image
kubectl set image deployment/technical-analysis \
  technical-analysis=ghcr.io/gravitywavesml/gravity_techanalysis:v1.1.0 \
  -n tech-analysis-prod

# Ù†Ø¸Ø§Ø±Øª Ø¨Ø± rollout
kubectl rollout status deployment/technical-analysis -n tech-analysis-prod

# Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ú©Ù„ØŒ rollback
kubectl rollout undo deployment/technical-analysis -n tech-analysis-prod
```

### Canary Deployment
```bash
# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø³Ø®Ù‡ canary
kubectl apply -f k8s/deployment-canary.yaml

# ØªÙ†Ø¸ÛŒÙ… traffic split (20% canary)
kubectl apply -f k8s/virtual-service-canary.yaml

# Ù…Ø§Ù†ÛŒØªÙˆØ± metrics Ø¨Ø±Ø§ÛŒ 30 Ø¯Ù‚ÛŒÙ‚Ù‡
# Ø§Ú¯Ø± Ù…ÙˆÙÙ‚: promote canary
kubectl apply -f k8s/deployment-canary-full.yaml

# Ø§Ú¯Ø± Ù†Ø§Ù…ÙˆÙÙ‚: rollback
kubectl delete -f k8s/deployment-canary.yaml
```

---

## ğŸ“Š Monitoring Ùˆ Alerts

### Key Metrics

#### Application Metrics
```prometheus
# Request Rate
rate(http_requests_total[5m])

# Error Rate
rate(http_requests_total{status=~"5.."}[5m])

# Response Time (p95)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Active Requests
http_requests_active

# Cache Hit Rate
rate(cache_hits_total[5m]) / rate(cache_requests_total[5m])
```

#### System Metrics
```prometheus
# CPU Usage
container_cpu_usage_seconds_total

# Memory Usage
container_memory_usage_bytes

# Pod Restart Count
kube_pod_container_status_restarts_total

# Replica Count
kube_deployment_status_replicas_available
```

### Alert Rules

#### Critical Alerts (P1)
```yaml
# High Error Rate
alert: HighErrorRate
expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
for: 2m
severity: critical
action: "Page on-call engineer"

# Service Down
alert: ServiceDown
expr: up{job="technical-analysis"} == 0
for: 1m
severity: critical
action: "Immediate investigation"

# Pod Crash Loop
alert: PodCrashLooping
expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
severity: critical
action: "Check pod logs"
```

#### Warning Alerts (P2)
```yaml
# High Response Time
alert: HighResponseTime
expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
for: 5m
severity: warning
action: "Monitor for scaling"

# Redis Connection Issues
alert: RedisCacheDown
expr: redis_up == 0
for: 2m
severity: warning
action: "Check Redis cluster health"

# Low Cache Hit Rate
alert: LowCacheHitRate
expr: rate(cache_hits_total[5m]) / rate(cache_requests_total[5m]) < 0.5
for: 10m
severity: warning
action: "Review cache configuration"
```

### Dashboards

#### Grafana Dashboard IDs
- **Overview**: `tech-analysis-overview`
- **Performance**: `tech-analysis-performance`
- **Business Metrics**: `tech-analysis-business`
- **Infrastructure**: `tech-analysis-infrastructure`

#### Key Panels
1. Request Rate (requests/sec)
2. Error Rate (%)
3. Response Time (p50, p95, p99)
4. CPU & Memory Usage
5. Cache Hit Rate
6. Active Connections
7. Top Endpoints
8. Error Breakdown by Type

---

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. **Service Not Responding (503)**

**Ø¹Ù„Ø§Ø¦Ù…:**
- Health checks failing
- 503 errors Ù…Ù† load balancer
- Pods Ø¯Ø± Ø­Ø§Ù„Øª `CrashLoopBackOff`

**ØªØ´Ø®ÛŒØµ:**
```bash
# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª pods
kubectl get pods -n tech-analysis-prod

# Ø¨Ø±Ø±Ø³ÛŒ logs
kubectl logs -f pod/technical-analysis-xxx -n tech-analysis-prod

# Ø¨Ø±Ø±Ø³ÛŒ events
kubectl describe pod/technical-analysis-xxx -n tech-analysis-prod

# Ø¨Ø±Ø±Ø³ÛŒ health endpoint
kubectl port-forward svc/technical-analysis-service 8000:8000
curl http://localhost:8000/health/ready
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```bash
# Ø§Ú¯Ø± Redis down Ø§Ø³Øª:
kubectl get pods -l app=redis -n tech-analysis-prod
kubectl logs -f redis-master-0 -n tech-analysis-prod

# Ø§Ú¯Ø± Ù…Ø´Ú©Ù„ configuration Ø§Ø³Øª:
kubectl get configmap technical-analysis-config -o yaml
kubectl edit configmap technical-analysis-config

# restart pods
kubectl rollout restart deployment/technical-analysis -n tech-analysis-prod
```

---

#### 2. **High Response Time (> 2s)**

**Ø¹Ù„Ø§Ø¦Ù…:**
- p95 latency > 2 seconds
- Client timeouts
- Queue buildup

**ØªØ´Ø®ÛŒØµ:**
```bash
# Ø¨Ø±Ø±Ø³ÛŒ CPU/Memory
kubectl top pods -n tech-analysis-prod

# Ø¨Ø±Ø±Ø³ÛŒ HPA
kubectl get hpa -n tech-analysis-prod

# Ø¨Ø±Ø±Ø³ÛŒ slow queries Ø¯Ø± logs
kubectl logs -f deployment/technical-analysis \
  -n tech-analysis-prod | grep "duration"
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```bash
# Ø§ÙØ²Ø§ÛŒØ´ replicas (Ø§Ú¯Ø± CPU Ø¨Ø§Ù„Ø§Ø³Øª)
kubectl scale deployment/technical-analysis --replicas=10 -n tech-analysis-prod

# Ø¨Ø±Ø±Ø³ÛŒ cache
redis-cli info stats | grep keyspace_hits

# Ø§Ú¯Ø± cache miss rate Ø¨Ø§Ù„Ø§Ø³Øª:
# Ø§ÙØ²Ø§ÛŒØ´ TTL ÛŒØ§ review Ú©Ø±Ø¯Ù† cache strategy

# Ø§Ú¯Ø± database slow Ø§Ø³Øª:
# Ø¨Ø±Ø±Ø³ÛŒ indexes Ùˆ queries
```

---

#### 3. **Memory Leak / OOM Kills**

**Ø¹Ù„Ø§Ø¦Ù…:**
- Pods restart frequently
- OOMKilled status
- Memory usage Ø±Ùˆ Ø¨Ù‡ Ø§ÙØ²Ø§ÛŒØ´

**ØªØ´Ø®ÛŒØµ:**
```bash
# Ø¨Ø±Ø±Ø³ÛŒ memory usage history
kubectl top pods -n tech-analysis-prod --watch

# Ø¨Ø±Ø±Ø³ÛŒ OOM kills
kubectl get events -n tech-analysis-prod | grep OOM

# Ù¾Ø±ÙˆÙØ§ÛŒÙ„ memory
kubectl exec -it technical-analysis-xxx -n tech-analysis-prod -- sh
# Ø¯Ø± container:
pip install memory_profiler
python -m memory_profiler main.py
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```bash
# Ø§ÙØ²Ø§ÛŒØ´ memory limits (Ù…ÙˆÙ‚Øª)
kubectl edit deployment technical-analysis -n tech-analysis-prod
# resources.limits.memory: 4Gi

# Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ leaks:
# - Connection pools not closed
# - Large objects in memory
# - Circular references

# Redeploy Ø¨Ø§ fix
```

---

#### 4. **Redis Connection Errors**

**Ø¹Ù„Ø§Ø¦Ù…:**
- `ConnectionError: Error connecting to Redis`
- Cache misses Ø²ÛŒØ§Ø¯
- Timeouts

**ØªØ´Ø®ÛŒØµ:**
```bash
# Ø¨Ø±Ø±Ø³ÛŒ Redis health
kubectl exec -it redis-master-0 -n tech-analysis-prod -- redis-cli ping

# Ø¨Ø±Ø±Ø³ÛŒ connections
kubectl exec -it redis-master-0 -n tech-analysis-prod -- redis-cli info clients

# Ø¨Ø±Ø±Ø³ÛŒ network
kubectl exec -it technical-analysis-xxx -n tech-analysis-prod -- \
  telnet redis-service 6379
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```bash
# Ø§Ú¯Ø± Redis down Ø§Ø³Øª:
kubectl rollout restart statefulset redis -n tech-analysis-prod

# Ø§Ú¯Ø± connection pool exhausted Ø§Ø³Øª:
# Ø§ÙØ²Ø§ÛŒØ´ max_connections Ø¯Ø± config

# Ø§Ú¯Ø± network issue Ø§Ø³Øª:
kubectl get networkpolicies -n tech-analysis-prod
```

---

#### 5. **High Error Rate (5xx)**

**Ø¹Ù„Ø§Ø¦Ù…:**
- 500/503 errors spike
- Alert fired
- User complaints

**ØªØ´Ø®ÛŒØµ:**
```bash
# Ø¨Ø±Ø±Ø³ÛŒ error logs
kubectl logs -f deployment/technical-analysis \
  -n tech-analysis-prod | grep ERROR

# Ø¨Ø±Ø±Ø³ÛŒ error breakdown
# Ø¯Ø± Grafana ÛŒØ§ Kibana

# trace errors
# Ø¯Ø± Jaeger UI
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```bash
# Ø§Ú¯Ø± dependency down Ø§Ø³Øª:
kubectl get svc -n tech-analysis-prod

# Ø§Ú¯Ø± bug Ø¯Ø± code Ø§Ø³Øª:
# Hotfix & redeploy

# Ø§Ú¯Ø± rate limit Ø§Ø³Øª:
# Ø§ÙØ²Ø§ÛŒØ´ limits ÛŒØ§ scaling

# Ø¯Ø± Ø¶Ø±ÙˆØ±Øª: rollback
kubectl rollout undo deployment/technical-analysis -n tech-analysis-prod
```

---

## ğŸ’¾ Backup Ùˆ Recovery

### Backup Strategy

#### 1. Configuration Backup
```bash
# Daily backup Ù‡Ù…Ù‡ configs
kubectl get all,configmap,secret -n tech-analysis-prod -o yaml > backup-$(date +%Y%m%d).yaml

# Automated backup
# Ø¯Ø± CronJob
```

#### 2. Redis Backup
```bash
# Manual backup
kubectl exec redis-master-0 -n tech-analysis-prod -- redis-cli SAVE

# Copy RDB file
kubectl cp redis-master-0:/data/dump.rdb ./redis-backup-$(date +%Y%m%d).rdb

# Automated: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Redis backup tool ÛŒØ§ Velero
```

### Recovery Procedures

#### Disaster Recovery
```bash
# 1. Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø§Ø² backup
kubectl apply -f backup-20240101.yaml

# 2. Restore Redis
kubectl cp redis-backup.rdb redis-master-0:/data/dump.rdb
kubectl exec redis-master-0 -- redis-cli SHUTDOWN
# Redis auto-restart Ùˆ load Ù…ÛŒâ€ŒÚ©Ù†Ø¯

# 3. Ø¨Ø±Ø±Ø³ÛŒ health
kubectl get pods -n tech-analysis-prod
```

---

## ğŸ“ˆ Scaling Strategies

### Horizontal Scaling (HPA)
```yaml
# Auto-scaling based on CPU & Memory
Current: 3-20 replicas
Triggers:
  - CPU > 70%: scale up
  - Memory > 80%: scale up
  - Request rate > 1000/s: scale up
```

### Manual Scaling
```bash
# Scale up
kubectl scale deployment/technical-analysis --replicas=15 -n tech-analysis-prod

# Scale down (off-peak hours)
kubectl scale deployment/technical-analysis --replicas=5 -n tech-analysis-prod
```

### Vertical Scaling
```bash
# Ø§ÙØ²Ø§ÛŒØ´ resources
kubectl edit deployment technical-analysis -n tech-analysis-prod

# Update:
resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 4000m
    memory: 4Gi
```

---

## ğŸ”’ Security Procedures

### Security Checklist
- [ ] ØªÙ…Ø§Ù… secrets Ø¯Ø± Vault
- [ ] TLS Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… connections
- [ ] Network policies ÙØ¹Ø§Ù„
- [ ] RBAC Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡
- [ ] Security headers ÙØ¹Ø§Ù„
- [ ] Rate limiting ÙØ¹Ø§Ù„
- [ ] Input validation ÙØ¹Ø§Ù„
- [ ] Audit logging ÙØ¹Ø§Ù„

### Incident Response
1. **ØªØ´Ø®ÛŒØµ**: Alerts â†’ PagerDuty
2. **ØªØ­Ù„ÛŒÙ„**: Logs + Traces
3. **Contain**: Isolate affected pods
4. **Recover**: Redeploy/Rollback
5. **Post-mortem**: Document lessons

### Security Updates
```bash
# Ø¨Ø±Ø±Ø³ÛŒ vulnerabilities
trivy image ghcr.io/gravitywavesml/gravity_techanalysis:latest

# Update dependencies
pip list --outdated
pip install --upgrade <package>

# Redeploy
kubectl rollout restart deployment/technical-analysis -n tech-analysis-prod
```

---

## ğŸ“ On-Call Contact

| Role | Contact | Escalation |
|------|---------|------------|
| Primary On-Call | +1-xxx-xxx-xxxx | 5 min |
| Secondary On-Call | +1-xxx-xxx-xxxx | 15 min |
| Team Lead | +1-xxx-xxx-xxxx | 30 min |
| Director | +1-xxx-xxx-xxxx | 1 hour |

---

## ğŸ“š Ù…Ø±Ø§Ø¬Ø¹

- **API Documentation**: https://api.example.com/docs
- **Grafana**: https://grafana.example.com
- **Jaeger**: https://jaeger.example.com
- **Kubernetes Dashboard**: https://k8s.example.com
- **Runbook Updates**: [GitHub Wiki](https://github.com/...)
