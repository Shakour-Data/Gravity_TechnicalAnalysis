"""
Database Manager with Auto-Setup and Fallback

Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
1. Auto-setup: Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ø¬Ø¯Ø§ÙˆÙ„
2. Fallback: Ú©Ø§Ø± Ø¨Ø¯ÙˆÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (in-memory SQLite ÛŒØ§ JSON)
3. Migration: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ schema Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø±

Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:
- Ø§Ú¯Ø± PostgreSQL Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ â†’ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² PostgreSQL
- Ø§Ú¯Ø± PostgreSQL Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ â†’ fallback Ø¨Ù‡ SQLite in-memory
- Ø§Ú¯Ø± SQLite Ù‡Ù… Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ â†’ fallback Ø¨Ù‡ JSON file storage

Author: Gravity Tech Team
Date: November 14, 2025
Version: 1.0.0
License: MIT
"""

import os
import json
import sqlite3
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime, timedelta
from enum import Enum
import logging

# Try to import PostgreSQL driver
try:
    import psycopg2
    from psycopg2 import pool
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("âš ï¸ psycopg2 not available. Will use SQLite fallback.")


logger = logging.getLogger(__name__)


class DatabaseType(str, Enum):
    """Ù†ÙˆØ¹ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    POSTGRESQL = "postgresql"
    SQLITE = "sqlite"
    JSON_FILE = "json_file"


class DatabaseManager:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ auto-setup Ùˆ fallback
    
    ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
    - Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø§Ø®Øª schema
    - Fallback Ø¨Ù‡ SQLite ÛŒØ§ JSON
    - Migration Ø®ÙˆØ¯Ú©Ø§Ø±
    - Connection pooling
    """
    
    def __init__(
        self,
        db_type: Optional[DatabaseType] = None,
        connection_string: Optional[str] = None,
        sqlite_path: Optional[str] = None,
        json_path: Optional[str] = None,
        auto_setup: bool = True
    ):
        """
        Initialize Database Manager
        
        Args:
            db_type: Ù†ÙˆØ¹ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø§Ú¯Ø± None Ø¨Ø§Ø´Ø¯ØŒ Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯)
            connection_string: Ø±Ø´ØªÙ‡ Ø§ØªØµØ§Ù„ PostgreSQL
            sqlite_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ SQLite (Ø¨Ø±Ø§ÛŒ fallback)
            json_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ JSON (Ø¨Ø±Ø§ÛŒ fallback)
            auto_setup: Ø¢ÛŒØ§ Ø®ÙˆØ¯Ú©Ø§Ø± schema Ø±Ø§ Ø¨Ø³Ø§Ø²Ø¯ØŸ
        """
        self.db_type = db_type
        self.connection_string = connection_string
        self.sqlite_path = sqlite_path or "data/tool_performance.db"
        self.json_path = json_path or "data/tool_performance.json"
        
        self.connection_pool = None
        self.sqlite_connection = None
        self.json_data = {}
        
        # Auto-detect database type
        if self.db_type is None:
            self.db_type = self._detect_database_type()
        
        # Initialize database
        self._initialize_database()
        
        # Auto-setup schema
        if auto_setup:
            self.setup_schema()
        
        logger.info(f"âœ… Database initialized: {self.db_type.value}")
    
    def _detect_database_type(self) -> DatabaseType:
        """
        ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ù†ÙˆØ¹ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        
        Ø§ÙˆÙ„ÙˆÛŒØª:
        1. PostgreSQL (Ø§Ú¯Ø± psycopg2 Ùˆ connection_string Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        2. SQLite (fallback)
        3. JSON file (fallback Ù†Ù‡Ø§ÛŒÛŒ)
        """
        
        # Check for PostgreSQL
        if POSTGRES_AVAILABLE and self.connection_string:
            try:
                # Test connection
                conn = psycopg2.connect(self.connection_string)
                conn.close()
                logger.info("âœ… PostgreSQL detected and available")
                return DatabaseType.POSTGRESQL
            except Exception as e:
                logger.warning(f"âš ï¸ PostgreSQL connection failed: {e}")
        
        # Check for environment variable
        postgres_url = os.getenv("DATABASE_URL") or os.getenv("POSTGRES_URL")
        if POSTGRES_AVAILABLE and postgres_url:
            try:
                conn = psycopg2.connect(postgres_url)
                conn.close()
                self.connection_string = postgres_url
                logger.info("âœ… PostgreSQL from env variable")
                return DatabaseType.POSTGRESQL
            except Exception as e:
                logger.warning(f"âš ï¸ PostgreSQL from env failed: {e}")
        
        # Fallback to SQLite
        try:
            # Create directory if needed
            Path(self.sqlite_path).parent.mkdir(parents=True, exist_ok=True)
            conn = sqlite3.connect(self.sqlite_path)
            conn.close()
            logger.info("âœ… SQLite fallback activated")
            return DatabaseType.SQLITE
        except Exception as e:
            logger.warning(f"âš ï¸ SQLite failed: {e}")
        
        # Final fallback to JSON
        logger.info("âœ… JSON file fallback activated")
        return DatabaseType.JSON_FILE
    
    def _initialize_database(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ØªØµØ§Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        
        if self.db_type == DatabaseType.POSTGRESQL:
            self._initialize_postgresql()
        elif self.db_type == DatabaseType.SQLITE:
            self._initialize_sqlite()
        elif self.db_type == DatabaseType.JSON_FILE:
            self._initialize_json()
    
    def _initialize_postgresql(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ PostgreSQL Ø¨Ø§ connection pool"""
        try:
            self.connection_pool = psycopg2.pool.SimpleConnectionPool(
                minconn=1,
                maxconn=10,
                dsn=self.connection_string
            )
            logger.info("âœ… PostgreSQL connection pool created")
        except Exception as e:
            logger.error(f"âŒ Failed to create PostgreSQL pool: {e}")
            raise
    
    def _initialize_sqlite(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ SQLite"""
        try:
            Path(self.sqlite_path).parent.mkdir(parents=True, exist_ok=True)
            self.sqlite_connection = sqlite3.connect(
                self.sqlite_path,
                check_same_thread=False
            )
            self.sqlite_connection.row_factory = sqlite3.Row
            logger.info(f"âœ… SQLite initialized: {self.sqlite_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize SQLite: {e}")
            raise
    
    def _initialize_json(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ JSON file storage"""
        try:
            Path(self.json_path).parent.mkdir(parents=True, exist_ok=True)
            
            if Path(self.json_path).exists():
                with open(self.json_path, 'r', encoding='utf-8') as f:
                    self.json_data = json.load(f)
            else:
                self.json_data = {
                    "tool_performance_history": [],
                    "tool_performance_stats": [],
                    "ml_weights_history": [],
                    "tool_recommendations_log": []
                }
                self._save_json()
            
            logger.info(f"âœ… JSON storage initialized: {self.json_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize JSON: {e}")
            raise
    
    def setup_schema(self):
        """
        Ø³Ø§Ø®Øª Ø®ÙˆØ¯Ú©Ø§Ø± schema Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        
        Ø§ÛŒÙ† Ù…ØªØ¯ schema Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
        """
        
        if self.db_type == DatabaseType.POSTGRESQL:
            self._setup_postgresql_schema()
        elif self.db_type == DatabaseType.SQLITE:
            self._setup_sqlite_schema()
        elif self.db_type == DatabaseType.JSON_FILE:
            # JSON doesn't need schema
            logger.info("âœ… JSON schema (structure) ready")
    
    def _setup_postgresql_schema(self):
        """Ø³Ø§Ø®Øª schema Ø¨Ø±Ø§ÛŒ PostgreSQL"""
        
        schema_file = Path(__file__).parent / "tool_performance_history.sql"
        
        if not schema_file.exists():
            logger.warning(f"âš ï¸ Schema file not found: {schema_file}")
            return
        
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_sql = f.read()
            
            conn = self.connection_pool.getconn()
            cursor = conn.cursor()
            
            # Execute schema
            cursor.execute(schema_sql)
            conn.commit()
            
            cursor.close()
            self.connection_pool.putconn(conn)
            
            logger.info("âœ… PostgreSQL schema created successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to create PostgreSQL schema: {e}")
            # Don't raise - schema might already exist
    
    def _setup_sqlite_schema(self):
        """Ø³Ø§Ø®Øª schema Ø¨Ø±Ø§ÛŒ SQLite"""
        
        # SQLite version of schema (simplified, no functions/triggers)
        schema = """
        -- Tool Performance History
        CREATE TABLE IF NOT EXISTS tool_performance_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tool_name TEXT NOT NULL,
            tool_category TEXT NOT NULL,
            
            symbol TEXT NOT NULL,
            timeframe TEXT NOT NULL,
            market_regime TEXT NOT NULL,
            volatility_level REAL,
            trend_strength REAL,
            volume_profile TEXT,
            
            prediction_type TEXT NOT NULL,
            prediction_value REAL,
            confidence_score REAL,
            
            actual_result TEXT,
            actual_price_change REAL,
            success INTEGER,
            accuracy REAL,
            
            prediction_timestamp TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
            result_timestamp TEXT,
            evaluation_period_hours INTEGER,
            
            metadata TEXT,
            
            created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_tool_performance_tool_name 
            ON tool_performance_history(tool_name);
        CREATE INDEX IF NOT EXISTS idx_tool_performance_symbol 
            ON tool_performance_history(symbol);
        CREATE INDEX IF NOT EXISTS idx_tool_performance_regime 
            ON tool_performance_history(market_regime);
        
        -- Tool Performance Stats
        CREATE TABLE IF NOT EXISTS tool_performance_stats (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tool_name TEXT NOT NULL,
            tool_category TEXT NOT NULL,
            
            market_regime TEXT,
            timeframe TEXT,
            period_start TEXT NOT NULL,
            period_end TEXT NOT NULL,
            
            total_predictions INTEGER NOT NULL DEFAULT 0,
            correct_predictions INTEGER NOT NULL DEFAULT 0,
            accuracy REAL,
            
            avg_confidence REAL,
            avg_actual_change REAL,
            
            bullish_predictions INTEGER DEFAULT 0,
            bearish_predictions INTEGER DEFAULT 0,
            neutral_predictions INTEGER DEFAULT 0,
            
            bullish_success_rate REAL,
            bearish_success_rate REAL,
            neutral_success_rate REAL,
            
            best_accuracy REAL,
            worst_accuracy REAL,
            best_symbol TEXT,
            worst_symbol TEXT,
            
            last_updated TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
            
            UNIQUE(tool_name, market_regime, timeframe, period_start, period_end)
        );
        
        -- ML Weights History
        CREATE TABLE IF NOT EXISTS ml_weights_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            
            model_name TEXT NOT NULL,
            model_version TEXT NOT NULL,
            
            market_regime TEXT,
            timeframe TEXT,
            
            weights TEXT NOT NULL,
            
            training_accuracy REAL,
            validation_accuracy REAL,
            r2_score REAL,
            mae REAL,
            
            training_samples INTEGER,
            training_date TEXT NOT NULL,
            
            metadata TEXT,
            
            created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_ml_weights_model 
            ON ml_weights_history(model_name, model_version);
        
        -- Tool Recommendations Log
        CREATE TABLE IF NOT EXISTS tool_recommendations_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            
            request_id TEXT NOT NULL,
            user_id TEXT,
            
            symbol TEXT NOT NULL,
            timeframe TEXT NOT NULL,
            analysis_goal TEXT,
            trading_style TEXT,
            
            market_regime TEXT NOT NULL,
            volatility_level REAL,
            trend_strength REAL,
            
            recommended_tools TEXT NOT NULL,
            ml_weights TEXT,
            
            user_feedback TEXT,
            tools_actually_used TEXT,
            trade_result TEXT,
            
            created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
            feedback_at TEXT
        );
        
        CREATE INDEX IF NOT EXISTS idx_recommendations_request 
            ON tool_recommendations_log(request_id);
        """
        
        try:
            cursor = self.sqlite_connection.cursor()
            cursor.executescript(schema)
            self.sqlite_connection.commit()
            logger.info("âœ… SQLite schema created successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to create SQLite schema: {e}")
    
    def get_connection(self):
        """Ø¯Ø±ÛŒØ§ÙØª connection (Ø¨Ø±Ø§ÛŒ PostgreSQL ÛŒØ§ SQLite)"""
        
        if self.db_type == DatabaseType.POSTGRESQL:
            return self.connection_pool.getconn()
        elif self.db_type == DatabaseType.SQLITE:
            return self.sqlite_connection
        else:
            raise ValueError(f"get_connection not supported for {self.db_type}")
    
    def release_connection(self, conn):
        """Ø¢Ø²Ø§Ø¯ Ú©Ø±Ø¯Ù† connection (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ PostgreSQL)"""
        
        if self.db_type == DatabaseType.POSTGRESQL:
            self.connection_pool.putconn(conn)
    
    def execute_query(
        self,
        query: str,
        params: Optional[Tuple] = None,
        fetch: bool = False
    ) -> Optional[List]:
        """
        Ø§Ø¬Ø±Ø§ÛŒ query Ø¨Ø§ fallback
        
        Args:
            query: SQL query
            params: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ query
            fetch: Ø¢ÛŒØ§ Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯ØŸ
        
        Returns:
            Ù†ØªÛŒØ¬Ù‡ query (Ø§Ú¯Ø± fetch=True)
        """
        
        if self.db_type == DatabaseType.JSON_FILE:
            # JSON doesn't support SQL queries
            logger.warning("âš ï¸ SQL query not supported for JSON storage")
            return None
        
        try:
            conn = self.get_connection()
            cursor = conn.cursor()
            
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            if fetch:
                if self.db_type == DatabaseType.POSTGRESQL:
                    result = cursor.fetchall()
                else:  # SQLite
                    result = [dict(row) for row in cursor.fetchall()]
            else:
                result = None
            
            conn.commit()
            cursor.close()
            
            self.release_connection(conn)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Query execution failed: {e}")
            raise
    
    def record_tool_performance(
        self,
        tool_name: str,
        tool_category: str,
        symbol: str,
        timeframe: str,
        market_regime: str,
        prediction_type: str,
        confidence_score: float,
        volatility_level: Optional[float] = None,
        trend_strength: Optional[float] = None,
        volume_profile: Optional[str] = None,
        prediction_value: Optional[float] = None,
        actual_result: Optional[str] = None,
        actual_price_change: Optional[float] = None,
        success: Optional[bool] = None,
        accuracy: Optional[float] = None,
        result_timestamp=None,
        evaluation_period_hours: Optional[int] = None,
        metadata: Optional[Dict] = None
    ) -> int:
        """
        Ø«Ø¨Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ø¨Ø²Ø§Ø± (ØªÙ…Ø§Ù… ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
        
        Returns:
            ID Ø±Ú©ÙˆØ±Ø¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        """

        # Normalize datetime input if provided
        if result_timestamp is not None:
            try:
                result_timestamp = result_timestamp.isoformat()  # type: ignore[attr-defined]
            except Exception:
                pass
        
        if self.db_type == DatabaseType.JSON_FILE:
            return self._record_tool_performance_json(
                tool_name, tool_category, symbol, timeframe, market_regime,
                prediction_type, confidence_score, volatility_level,
                trend_strength, volume_profile, prediction_value,
                actual_result, actual_price_change, success, accuracy,
                result_timestamp, evaluation_period_hours, metadata
            )
        
        query = """
        INSERT INTO tool_performance_history (
            tool_name, tool_category, symbol, timeframe, market_regime,
            volatility_level, trend_strength, volume_profile,
            prediction_type, prediction_value, confidence_score,
            actual_result, actual_price_change, success, accuracy,
            result_timestamp, evaluation_period_hours, metadata
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        RETURNING id
        """ if self.db_type == DatabaseType.POSTGRESQL else """
        INSERT INTO tool_performance_history (
            tool_name, tool_category, symbol, timeframe, market_regime,
            volatility_level, trend_strength, volume_profile,
            prediction_type, prediction_value, confidence_score,
            actual_result, actual_price_change, success, accuracy,
            result_timestamp, evaluation_period_hours, metadata
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        
        metadata_json = json.dumps(metadata) if metadata else None
        
        params = (
            tool_name, tool_category, symbol, timeframe, market_regime,
            volatility_level, trend_strength, volume_profile,
            prediction_type, prediction_value, confidence_score,
            actual_result, actual_price_change, success, accuracy,
            result_timestamp, evaluation_period_hours, metadata_json
        )
        
        try:
            conn = self.get_connection()
            cursor = conn.cursor()
            cursor.execute(query, params)
            
            if self.db_type == DatabaseType.POSTGRESQL:
                record_id = cursor.fetchone()[0]
            else:  # SQLite
                record_id = cursor.lastrowid
            
            conn.commit()
            cursor.close()
            self.release_connection(conn)
            
            return record_id
            
        except Exception as e:
            logger.error(f"âŒ Failed to record performance: {e}")
            raise
    
    def update_tool_result(
        self,
        record_id: int,
        actual_result: Optional[str] = None,
        actual_price_change: Optional[float] = None,
        success: Optional[bool] = None,
        accuracy: Optional[float] = None,
        result_timestamp=None,
        evaluation_period_hours: Optional[int] = None
    ) -> bool:
        """
        Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±Ø§ÛŒ Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± tool_performance_history
        """
        if result_timestamp is not None:
            try:
                result_timestamp = result_timestamp.isoformat()  # type: ignore[attr-defined]
            except Exception:
                pass

        if self.db_type == DatabaseType.JSON_FILE:
            for item in self.json_data.get("tool_performance_history", []):
                if item.get("id") == record_id:
                    if actual_result is not None:
                        item["actual_result"] = actual_result
                    if actual_price_change is not None:
                        item["actual_price_change"] = actual_price_change
                    if success is not None:
                        item["success"] = success
                    if accuracy is not None:
                        item["accuracy"] = accuracy
                    if result_timestamp is not None:
                        item["result_timestamp"] = result_timestamp
                    if evaluation_period_hours is not None:
                        item["evaluation_period_hours"] = evaluation_period_hours
                    item["updated_at"] = datetime.utcnow().isoformat()
                    self._save_json()
                    return True
            return False

        set_clauses = []
        params: list[Any] = []
        placeholder = "%s" if self.db_type == DatabaseType.POSTGRESQL else "?"

        if actual_result is not None:
            set_clauses.append(f"actual_result = {placeholder}")
            params.append(actual_result)
        if actual_price_change is not None:
            set_clauses.append(f"actual_price_change = {placeholder}")
            params.append(actual_price_change)
        if success is not None:
            set_clauses.append(f"success = {placeholder}")
            params.append(success)
        if accuracy is not None:
            set_clauses.append(f"accuracy = {placeholder}")
            params.append(accuracy)
        if result_timestamp is not None:
            set_clauses.append(f"result_timestamp = {placeholder}")
            params.append(result_timestamp)
        if evaluation_period_hours is not None:
            set_clauses.append(f"evaluation_period_hours = {placeholder}")
            params.append(evaluation_period_hours)

        # always bump updated_at
        set_clauses.append("updated_at = NOW()" if self.db_type == DatabaseType.POSTGRESQL else "updated_at = CURRENT_TIMESTAMP")

        if not set_clauses:
            return False

        query = f"""
        UPDATE tool_performance_history
        SET {', '.join(set_clauses)}
        WHERE id = {placeholder}
        """
        params.append(record_id)

        try:
            conn = self.get_connection()
            cursor = conn.cursor()
            cursor.execute(query, tuple(params))
            conn.commit()
            updated = cursor.rowcount > 0
            cursor.close()
            self.release_connection(conn)
            return updated
        except Exception as e:
            logger.error(f"âŒ Failed to update tool result: {e}")
            raise
    
    def _record_tool_performance_json(
        self, tool_name, tool_category, symbol, timeframe, market_regime,
        prediction_type, confidence_score, volatility_level,
        trend_strength, volume_profile, prediction_value,
        actual_result, actual_price_change, success, accuracy,
        result_timestamp, evaluation_period_hours, metadata
    ) -> int:
        """Ø«Ø¨Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± JSON"""
        
        record = {
            "id": len(self.json_data["tool_performance_history"]) + 1,
            "tool_name": tool_name,
            "tool_category": tool_category,
            "symbol": symbol,
            "timeframe": timeframe,
            "market_regime": market_regime,
            "volatility_level": volatility_level,
            "trend_strength": trend_strength,
            "volume_profile": volume_profile,
            "prediction_type": prediction_type,
            "prediction_value": prediction_value,
            "confidence_score": confidence_score,
            "actual_result": actual_result,
            "actual_price_change": actual_price_change,
            "success": success,
            "accuracy": accuracy,
            "result_timestamp": result_timestamp,
            "evaluation_period_hours": evaluation_period_hours,
            "metadata": metadata,
            "prediction_timestamp": datetime.utcnow().isoformat(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        self.json_data["tool_performance_history"].append(record)
        self._save_json()
        
        return record["id"]
    
    def get_tool_accuracy(
        self,
        tool_name: str,
        market_regime: Optional[str] = None,
        days: int = 30
    ) -> Dict[str, Any]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¯Ù‚Øª ÛŒÚ© Ø§Ø¨Ø²Ø§Ø±
        
        Args:
            tool_name: Ù†Ø§Ù… Ø§Ø¨Ø²Ø§Ø±
            market_regime: Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            days: ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ø¢Ù…Ø§Ø± Ø¯Ù‚Øª
        """
        
        if self.db_type == DatabaseType.JSON_FILE:
            return self._get_tool_accuracy_json(tool_name, market_regime, days)
        
        query = """
        SELECT 
            tool_name,
            market_regime,
            COUNT(*) as total_predictions,
            SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as correct_predictions,
            AVG(CASE WHEN success = 1 THEN 1.0 ELSE 0.0 END) as accuracy,
            AVG(confidence_score) as avg_confidence
        FROM tool_performance_history
        WHERE tool_name = %s
          AND (%s IS NULL OR market_regime = %s)
          AND prediction_timestamp >= NOW() - INTERVAL '%s days'
          AND success IS NOT NULL
        GROUP BY tool_name, market_regime
        """ if self.db_type == DatabaseType.POSTGRESQL else """
        SELECT 
            tool_name,
            market_regime,
            COUNT(*) as total_predictions,
            SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as correct_predictions,
            AVG(CASE WHEN success = 1 THEN 1.0 ELSE 0.0 END) as accuracy,
            AVG(confidence_score) as avg_confidence
        FROM tool_performance_history
        WHERE tool_name = ?
          AND (? IS NULL OR market_regime = ?)
          AND datetime(prediction_timestamp) >= datetime('now', '-' || ? || ' days')
          AND success IS NOT NULL
        GROUP BY tool_name, market_regime
        """
        
        params = (tool_name, market_regime, market_regime, days) if self.db_type == DatabaseType.POSTGRESQL \
                 else (tool_name, market_regime, market_regime, days)
        
        results = self.execute_query(query, params, fetch=True)
        
        if results:
            return dict(results[0]) if isinstance(results[0], sqlite3.Row) else {
                "tool_name": results[0][0],
                "market_regime": results[0][1],
                "total_predictions": results[0][2],
                "correct_predictions": results[0][3],
                "accuracy": results[0][4],
                "avg_confidence": results[0][5]
            }
        
        return {
            "tool_name": tool_name,
            "market_regime": market_regime,
            "total_predictions": 0,
            "correct_predictions": 0,
            "accuracy": 0.0,
            "avg_confidence": 0.0
        }
    
    def _get_tool_accuracy_json(
        self, tool_name: str, market_regime: Optional[str], days: int
    ) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ù‚Øª Ø§Ø² JSON"""
        
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        records = [
            r for r in self.json_data["tool_performance_history"]
            if r["tool_name"] == tool_name
            and (market_regime is None or r["market_regime"] == market_regime)
            and r.get("success") is not None
            and datetime.fromisoformat(r["prediction_timestamp"]) >= cutoff_date
        ]
        
        total = len(records)
        correct = sum(1 for r in records if r.get("success"))
        accuracy = correct / total if total > 0 else 0.0
        avg_confidence = sum(r.get("confidence_score", 0) for r in records) / total if total > 0 else 0.0
        
        return {
            "tool_name": tool_name,
            "market_regime": market_regime,
            "total_predictions": total,
            "correct_predictions": correct,
            "accuracy": accuracy,
            "avg_confidence": avg_confidence
        }
    
    def _save_json(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± JSON file"""
        try:
            with open(self.json_path, 'w', encoding='utf-8') as f:
                json.dump(self.json_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ Failed to save JSON: {e}")
    
    def close(self):
        """Ø¨Ø³ØªÙ† Ø§ØªØµØ§Ù„Ø§Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        
        if self.db_type == DatabaseType.POSTGRESQL and self.connection_pool:
            self.connection_pool.closeall()
            logger.info("âœ… PostgreSQL connections closed")
        
        elif self.db_type == DatabaseType.SQLITE and self.sqlite_connection:
            self.sqlite_connection.close()
            logger.info("âœ… SQLite connection closed")
        
        elif self.db_type == DatabaseType.JSON_FILE:
            self._save_json()
            logger.info("âœ… JSON data saved")
    
    def __enter__(self):
        """Context manager support"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager cleanup"""
        self.close()


# CLI tool for database setup
def main():
    """
    Ø§Ø¨Ø²Ø§Ø± Ø®Ø· ÙØ±Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ setup Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    
    Usage:
        python database_manager.py
        python database_manager.py --type postgresql --connection "postgresql://..."
        python database_manager.py --type sqlite --path data/mydb.db
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="Database Setup Tool")
    parser.add_argument(
        "--type",
        choices=["postgresql", "sqlite", "json"],
        help="Ù†ÙˆØ¹ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø§Ú¯Ø± Ù…Ø´Ø®Øµ Ù†Ø´ÙˆØ¯ØŒ Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)"
    )
    parser.add_argument(
        "--connection",
        help="Ø±Ø´ØªÙ‡ Ø§ØªØµØ§Ù„ PostgreSQL"
    )
    parser.add_argument(
        "--sqlite-path",
        default="data/tool_performance.db",
        help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ SQLite"
    )
    parser.add_argument(
        "--json-path",
        default="data/tool_performance.json",
        help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ JSON"
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸ—„ï¸  Database Setup Tool")
    print("=" * 70)
    
    db_type = DatabaseType(args.type) if args.type else None
    
    try:
        with DatabaseManager(
            db_type=db_type,
            connection_string=args.connection,
            sqlite_path=args.sqlite_path,
            json_path=args.json_path,
            auto_setup=True
        ) as db:
            print(f"\nâœ… Database setup complete!")
            print(f"   Type: {db.db_type.value}")
            
            if db.db_type == DatabaseType.POSTGRESQL:
                print(f"   Connection: PostgreSQL")
            elif db.db_type == DatabaseType.SQLITE:
                print(f"   Path: {db.sqlite_path}")
            elif db.db_type == DatabaseType.JSON_FILE:
                print(f"   Path: {db.json_path}")
            
            print(f"\nâœ… Schema created successfully")
            print(f"   Tables: tool_performance_history, tool_performance_stats,")
            print(f"           ml_weights_history, tool_recommendations_log")
    
    except Exception as e:
        print(f"\nâŒ Setup failed: {e}")
        return 1
    
    print("\n" + "=" * 70)
    return 0


if __name__ == "__main__":
    exit(main())
